{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iez96bhnVzL"
      },
      "source": [
        "# Video Classification with a CNN-RNN Architecture\n",
        "\n",
        "**Author:** [Sayak Paul](https://twitter.com/RisingSayak)<br>\n",
        "**Date created:** 2021/05/28<br>\n",
        "**Last modified:** 2021/06/05<br>\n",
        "**Description:** Training a video classifier with transfer learning and a recurrent model on the UCF101 dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te-q1JPwnVzN"
      },
      "source": [
        "This example demonstrates video classification, an important use-case with\n",
        "applications in recommendations, security, and so on.\n",
        "We will be using the [UCF101 dataset](https://www.crcv.ucf.edu/data/UCF101.php)\n",
        "to build our video classifier. The dataset consists of videos categorized into different\n",
        "actions, like cricket shot, punching, biking, etc. This dataset is commonly used to\n",
        "build action recognizers, which are an application of video classification.\n",
        "\n",
        "A video consists of an ordered sequence of frames. Each frame contains *spatial*\n",
        "information, and the sequence of those frames contains *temporal* information. To model\n",
        "both of these aspects, we use a hybrid architecture that consists of convolutions\n",
        "(for spatial processing) as well as recurrent layers (for temporal processing).\n",
        "Specifically, we'll use a Convolutional Neural Network (CNN) and a Recurrent Neural\n",
        "Network (RNN) consisting of [GRU layers](https://keras.io/api/layers/recurrent_layers/gru/).\n",
        "This kind of hybrid architecture is popularly known as a **CNN-RNN**.\n",
        "\n",
        "This example requires TensorFlow 2.5 or higher, as well as TensorFlow Docs, which can be\n",
        "installed using the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "43YRlywunVzN",
        "outputId": "784ef229-e7e6-4f06-da20-6923a3c18c37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/tensorflow/docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9cpbNlYnVzO"
      },
      "source": [
        "## Data collection\n",
        "\n",
        "In order to keep the runtime of this example relatively short, we will be using a\n",
        "subsampled version of the original UCF101 dataset. You can refer to\n",
        "[this notebook](https://colab.research.google.com/github/sayakpaul/Action-Recognition-in-TensorFlow/blob/main/Data_Preparation_UCF101.ipynb)\n",
        "to know how the subsampling was done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-KheuJrEnVzO"
      },
      "outputs": [],
      "source": [
        "!wget -q https://git.io/JGc31 -O ucf101_top5.tar.gz\n",
        "!tar xf ucf101_top5.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w7PcSEPnVzO"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "qbMSqXK9npHW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "D-t0MAOgnVzP"
      },
      "outputs": [],
      "source": [
        "from tensorflow_docs.vis import embed\n",
        "from tensorflow import keras\n",
        "from imutils import paths\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import imageio\n",
        "import cv2\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnrGS-kXnVzP"
      },
      "source": [
        "## Define hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BEt0w4HXnVzP"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "\n",
        "MAX_SEQ_LENGTH = 20\n",
        "NUM_FEATURES = 2048"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EznJDHAJnVzQ"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yQjPtUPYnVzQ",
        "outputId": "3dbb6812-00b3-42fc-b37f-d9dd51531688",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total videos for training: 594\n",
            "Total videos for testing: 224\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-79e7dfe0-dfff-4abd-ae9e-73c550667f3c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_name</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>506</th>\n",
              "      <td>v_TennisSwing_g12_c03.avi</td>\n",
              "      <td>TennisSwing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>v_PlayingCello_g18_c01.avi</td>\n",
              "      <td>PlayingCello</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>368</th>\n",
              "      <td>v_ShavingBeard_g09_c03.avi</td>\n",
              "      <td>ShavingBeard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>v_CricketShot_g18_c05.avi</td>\n",
              "      <td>CricketShot</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>v_PlayingCello_g11_c02.avi</td>\n",
              "      <td>PlayingCello</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>v_CricketShot_g22_c06.avi</td>\n",
              "      <td>CricketShot</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>v_CricketShot_g14_c01.avi</td>\n",
              "      <td>CricketShot</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>532</th>\n",
              "      <td>v_TennisSwing_g16_c01.avi</td>\n",
              "      <td>TennisSwing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315</th>\n",
              "      <td>v_Punch_g19_c03.avi</td>\n",
              "      <td>Punch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>v_CricketShot_g22_c03.avi</td>\n",
              "      <td>CricketShot</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-79e7dfe0-dfff-4abd-ae9e-73c550667f3c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-79e7dfe0-dfff-4abd-ae9e-73c550667f3c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-79e7dfe0-dfff-4abd-ae9e-73c550667f3c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                     video_name           tag\n",
              "506   v_TennisSwing_g12_c03.avi   TennisSwing\n",
              "184  v_PlayingCello_g18_c01.avi  PlayingCello\n",
              "368  v_ShavingBeard_g09_c03.avi  ShavingBeard\n",
              "73    v_CricketShot_g18_c05.avi   CricketShot\n",
              "139  v_PlayingCello_g11_c02.avi  PlayingCello\n",
              "97    v_CricketShot_g22_c06.avi   CricketShot\n",
              "42    v_CricketShot_g14_c01.avi   CricketShot\n",
              "532   v_TennisSwing_g16_c01.avi   TennisSwing\n",
              "315         v_Punch_g19_c03.avi         Punch\n",
              "94    v_CricketShot_g22_c03.avi   CricketShot"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "print(f\"Total videos for training: {len(train_df)}\")\n",
        "print(f\"Total videos for testing: {len(test_df)}\")\n",
        "\n",
        "train_df.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGaqqkK2nVzQ"
      },
      "source": [
        "One of the many challenges of training video classifiers is figuring out a way to feed\n",
        "the videos to a network. [This blog post](https://blog.coast.ai/five-video-classification-methods-implemented-in-keras-and-tensorflow-99cad29cc0b5)\n",
        "discusses five such methods. Since a video is an ordered sequence of frames, we could\n",
        "just extract the frames and put them in a 3D tensor. But the number of frames may differ\n",
        "from video to video which would prevent us from stacking them into batches\n",
        "(unless we use padding). As an alternative, we can **save video frames at a fixed\n",
        "interval until a maximum frame count is reached**. In this example we will do\n",
        "the following:\n",
        "\n",
        "1. Capture the frames of a video.\n",
        "2. Extract frames from the videos until a maximum frame count is reached.\n",
        "3. In the case, where a video's frame count is lesser than the maximum frame count we\n",
        "will pad the video with zeros.\n",
        "\n",
        "Note that this workflow is identical to [problems involving texts sequences](https://developers.google.com/machine-learning/guides/text-classification/). Videos of the UCF101 dataset is [known](https://www.crcv.ucf.edu/papers/UCF101_CRCV-TR-12-01.pdf)\n",
        "to not contain extreme variations in objects and actions across frames. Because of this,\n",
        "it may be okay to only consider a few frames for the learning task. But this approach may\n",
        "not generalize well to other video classification problems. We will be using\n",
        "[OpenCV's `VideoCapture()` method](https://docs.opencv.org/master/dd/d43/tutorial_py_video_display.html)\n",
        "to read frames from videos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1vIRcFsanVzR"
      },
      "outputs": [],
      "source": [
        "# The following two methods are taken from this tutorial:\n",
        "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
        "\n",
        "\n",
        "def crop_center_square(frame):\n",
        "    y, x = frame.shape[0:2]\n",
        "    min_dim = min(y, x)\n",
        "    start_x = (x // 2) - (min_dim // 2)\n",
        "    start_y = (y // 2) - (min_dim // 2)\n",
        "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
        "\n",
        "\n",
        "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = crop_center_square(frame)\n",
        "            frame = cv2.resize(frame, resize)\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEnWFUzynVzR"
      },
      "source": [
        "We can use a pre-trained network to extract meaningful features from the extracted\n",
        "frames. The [`Keras Applications`](https://keras.io/api/applications/) module provides\n",
        "a number of state-of-the-art models pre-trained on the [ImageNet-1k dataset](http://image-net.org/).\n",
        "We will be using the [InceptionV3 model](https://arxiv.org/abs/1512.00567) for this purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4Sw6uRMnnVzR",
        "outputId": "b0177359-45ee-42cc-915e-985319705444",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 2s 0us/step\n",
            "87924736/87910968 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def build_feature_extractor():\n",
        "    feature_extractor = keras.applications.InceptionV3(\n",
        "        weights=\"imagenet\",\n",
        "        include_top=False,\n",
        "        pooling=\"avg\",\n",
        "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    )\n",
        "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
        "\n",
        "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    preprocessed = preprocess_input(inputs)\n",
        "\n",
        "    outputs = feature_extractor(preprocessed)\n",
        "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
        "\n",
        "\n",
        "feature_extractor = build_feature_extractor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umpmuQMHnVzR"
      },
      "source": [
        "The labels of the videos are strings. Neural networks do not understand string values,\n",
        "so they must be converted to some numerical form before they are fed to the model. Here\n",
        "we will use the [`StringLookup`](https://keras.io/api/layers/preprocessing_layers/categorical/string_lookup)\n",
        "layer encode the class labels as integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HhQJMq0fnVzS",
        "outputId": "b929cf30-8b26-4aef-914e-8f0f26728978",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['CricketShot', 'PlayingCello', 'Punch', 'ShavingBeard', 'TennisSwing']\n"
          ]
        }
      ],
      "source": [
        "label_processor = keras.layers.StringLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"])\n",
        ")\n",
        "print(label_processor.get_vocabulary())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The following two methods are taken from this tutorial:\n",
        "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
        "\n",
        "\n",
        "def crop_center_square(frame):\n",
        "    y, x = frame.shape[0:2]\n",
        "    min_dim = min(y, x)\n",
        "    start_x = (x // 2) - (min_dim // 2)\n",
        "    start_y = (y // 2) - (min_dim // 2)\n",
        "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
        "\n",
        "\n",
        "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE),num_part=6):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    num_diff = int(np.floor(length/num_part))\n",
        "    frames = []\n",
        "    try:\n",
        "        for i in range(0,length,num_diff):\n",
        "            cap.set(1,i)\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = crop_center_square(frame)\n",
        "            frame = cv2.resize(frame, resize)\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "def label_to_tensor(st):\n",
        "    f_list = list(st)\n",
        "    f_list.sort()\n",
        "    st_list = []\n",
        "    for s in st:\n",
        "        s_index = f_list.index(s)\n",
        "        st_list.append(s_index)\n",
        "    return tf.convert_to_tensor(st_list, dtype=tf.float32)\n"
      ],
      "metadata": {
        "id": "ML-n8CKGn81H"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = os.getcwd()+'/train'\n",
        "num_samples = len(train_df)\n",
        "video_paths = train_df[\"video_name\"].values.tolist()\n",
        "labels = train_df[\"tag\"].values\n",
        "labels = label_to_tensor(labels[..., None]).numpy()\n",
        "\n",
        "frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
        "frame_features = np.zeros(\n",
        "    shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "frames= []\n",
        "for idx, path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames.append(load_video(os.path.join(root_dir, path)))"
      ],
      "metadata": {
        "id": "3Ljcoy4in-rE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_train_array = np.expand_dims(frames[0][:6,:,:,:],0)\n",
        "for i in frames[1:]:\n",
        "    my_train_array = np.vstack([my_train_array,np.expand_dims(i[:6,:,:,:],0)])"
      ],
      "metadata": {
        "id": "fJ5E_buwoT_D"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_dict={0:0, 118: 1, 238:2, 359:3, 477:4}\n",
        "new_train_label = np.vectorize(map_dict.get)(labels)\n",
        "depth = 5\n",
        "out_vec = tf.one_hot(new_train_label, depth)\n",
        "tr_df=tf.expand_dims(my_train_array, 1)\n",
        "def shuffle_split_data(X, y):\n",
        "    arr_rand = np.random.rand(X.shape[0])\n",
        "    split = arr_rand < np.percentile(arr_rand, 80)\n",
        "\n",
        "    X_train = X[split]\n",
        "    y_train = y[split]\n",
        "    X_test =  X[~split]\n",
        "    y_test = y[~split]\n",
        "\n",
        "    print(len(X_train), len(y_train), len(X_test), len(y_test))\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "X_Train,y_Train,X_Test,y_Test = shuffle_split_data(my_train_array, new_train_label)\n",
        "\n"
      ],
      "metadata": {
        "id": "VqG3cY06oZ6w",
        "outputId": "ea8d41c9-b57f-415a-8754-6a17bfcefc9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "475 475 119 119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 222\n",
        "\n",
        "feature_extractor = tf.keras.applications.InceptionV3(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False,\n",
        "    pooling=\"avg\",\n",
        "    \n",
        ")\n",
        "for i in range(20):\n",
        "    feature_extractor.layers[-i].trainable = False\n",
        "\n",
        "\n",
        "inputs = tf.keras.Input((6,224,224,3)) \n",
        "level1 = tf.keras.layers.Conv3D(27, 3, activation='relu')(inputs)\n",
        "level2 = tf.keras.layers.MaxPool3D((3,3,3))(level1)\n",
        "level3 = tf.keras.layers.Reshape((222,222,3))(level2)\n",
        "level4 = tf.keras.layers.Conv2D(100,3,activation='relu')(level3)\n",
        "level5 = tf.keras.layers.MaxPool2D((3,3))(level4)\n",
        "level6 = tf.keras.layers.Conv2D(10,3,activation='relu')(level5)\n",
        "level7 = tf.keras.layers.MaxPool2D((3,3))(level6)\n",
        "level8 = tf.keras.layers.Flatten()(level7)\n",
        "# preprocessed = feature_extractor(level3)\n",
        "# outputs = feature_extractor(preprocessed)\n",
        "\n",
        "output = tf.keras.layers.Dense(5, activation='softmax')(level8)\n",
        "\n",
        "final_model = tf.keras.Model(inputs, output, name=\"feature_extractor\")"
      ],
      "metadata": {
        "id": "ViHAR0LiozO_"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "filepath = 'tf1_mnist_cnn.hdf5'\n",
        "save_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, \\\n",
        "                             save_best_only=True, save_weights_only=False, \\\n",
        "                             mode='auto', period=1)\n"
      ],
      "metadata": {
        "id": "Yc-lmrkBsL96",
        "outputId": "a2436cef-f77c-4c8e-a4c0-f0048c080c33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "final_model.fit(x=X_Train,y=y_Train,validation_data = (X_Test,y_Test),verbose=1,epochs=150,callbacks=[save_checkpoint])"
      ],
      "metadata": {
        "id": "MD-uA1CHop9e",
        "outputId": "9f92b254-72c5-4750-decc-5bbae08aa478",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.2297 - accuracy: 0.9411\n",
            "Epoch 1: val_accuracy improved from -inf to 0.77311, saving model to tf1_mnist_cnn.hdf5\n",
            "15/15 [==============================] - 8s 473ms/step - loss: 0.2297 - accuracy: 0.9411 - val_loss: 1.6492 - val_accuracy: 0.7731\n",
            "Epoch 2/150\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.2524 - accuracy: 0.9158\n",
            "Epoch 2: val_accuracy improved from 0.77311 to 0.80672, saving model to tf1_mnist_cnn.hdf5\n",
            "15/15 [==============================] - 7s 462ms/step - loss: 0.2524 - accuracy: 0.9158 - val_loss: 1.0533 - val_accuracy: 0.8067\n",
            "Epoch 3/150\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1346 - accuracy: 0.9621\n",
            "Epoch 3: val_accuracy did not improve from 0.80672\n",
            "15/15 [==============================] - 7s 457ms/step - loss: 0.1346 - accuracy: 0.9621 - val_loss: 1.0690 - val_accuracy: 0.7815\n",
            "Epoch 4/150\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1026 - accuracy: 0.9684\n",
            "Epoch 4: val_accuracy did not improve from 0.80672\n",
            "15/15 [==============================] - 7s 457ms/step - loss: 0.1026 - accuracy: 0.9684 - val_loss: 1.6135 - val_accuracy: 0.7647\n",
            "Epoch 5/150\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0376 - accuracy: 0.9916\n",
            "Epoch 5: val_accuracy improved from 0.80672 to 0.83193, saving model to tf1_mnist_cnn.hdf5\n",
            "15/15 [==============================] - 7s 462ms/step - loss: 0.0376 - accuracy: 0.9916 - val_loss: 1.5766 - val_accuracy: 0.8319\n",
            "Epoch 6/150\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0389 - accuracy: 0.9916\n",
            "Epoch 6: val_accuracy did not improve from 0.83193\n",
            "15/15 [==============================] - 7s 458ms/step - loss: 0.0389 - accuracy: 0.9916 - val_loss: 1.7943 - val_accuracy: 0.7479\n",
            "Epoch 7/150\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 0.9958\n",
            "Epoch 7: val_accuracy did not improve from 0.83193\n",
            "15/15 [==============================] - 7s 459ms/step - loss: 0.0284 - accuracy: 0.9958 - val_loss: 1.4687 - val_accuracy: 0.8067\n",
            "Epoch 8/150\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9895\n",
            "Epoch 8: val_accuracy did not improve from 0.83193\n",
            "15/15 [==============================] - 7s 458ms/step - loss: 0.0293 - accuracy: 0.9895 - val_loss: 1.6656 - val_accuracy: 0.7815\n",
            "Epoch 9/150\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0637 - accuracy: 0.9853\n",
            "Epoch 9: val_accuracy did not improve from 0.83193\n",
            "15/15 [==============================] - 7s 458ms/step - loss: 0.0637 - accuracy: 0.9853 - val_loss: 1.7700 - val_accuracy: 0.7815\n",
            "Epoch 10/150\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0353 - accuracy: 0.9895\n",
            "Epoch 10: val_accuracy did not improve from 0.83193\n",
            "15/15 [==============================] - 7s 458ms/step - loss: 0.0353 - accuracy: 0.9895 - val_loss: 2.2773 - val_accuracy: 0.7647\n",
            "Epoch 11/150\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9916\n",
            "Epoch 11: val_accuracy did not improve from 0.83193\n",
            "15/15 [==============================] - 8s 506ms/step - loss: 0.0195 - accuracy: 0.9916 - val_loss: 1.6606 - val_accuracy: 0.8067\n",
            "Epoch 12/150\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9979\n",
            "Epoch 12: val_accuracy did not improve from 0.83193\n",
            "15/15 [==============================] - 7s 458ms/step - loss: 0.0144 - accuracy: 0.9979 - val_loss: 1.7047 - val_accuracy: 0.7731\n",
            "Epoch 13/150\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.9979\n",
            "Epoch 13: val_accuracy did not improve from 0.83193\n",
            "15/15 [==============================] - 7s 459ms/step - loss: 0.0086 - accuracy: 0.9979 - val_loss: 2.0024 - val_accuracy: 0.8319\n",
            "Epoch 14/150\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0030 - accuracy: 0.9979\n",
            "Epoch 14: val_accuracy did not improve from 0.83193\n",
            "15/15 [==============================] - 7s 458ms/step - loss: 0.0030 - accuracy: 0.9979 - val_loss: 2.0234 - val_accuracy: 0.8151\n",
            "Epoch 15/150\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 1.0000\n",
            "Epoch 15: val_accuracy did not improve from 0.83193\n",
            "15/15 [==============================] - 7s 459ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 1.9156 - val_accuracy: 0.7815\n",
            "Epoch 16/150\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 16: val_accuracy did not improve from 0.83193\n",
            "15/15 [==============================] - 7s 456ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.9638 - val_accuracy: 0.8067\n",
            "Epoch 17/150\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 17: val_accuracy did not improve from 0.83193\n",
            "15/15 [==============================] - 7s 459ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 2.2690 - val_accuracy: 0.8067\n",
            "Epoch 18/150\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 18: val_accuracy did not improve from 0.83193\n",
            "15/15 [==============================] - 7s 457ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.1404 - val_accuracy: 0.8151\n",
            "Epoch 19/150\n",
            "15/15 [==============================] - ETA: 0s - loss: 4.5900e-04 - accuracy: 1.0000\n",
            "Epoch 19: val_accuracy did not improve from 0.83193\n",
            "15/15 [==============================] - 7s 459ms/step - loss: 4.5900e-04 - accuracy: 1.0000 - val_loss: 2.0945 - val_accuracy: 0.8235\n",
            "Epoch 20/150\n",
            "15/15 [==============================] - ETA: 0s - loss: 2.8027e-04 - accuracy: 1.0000\n",
            "Epoch 20: val_accuracy did not improve from 0.83193\n",
            "15/15 [==============================] - 7s 458ms/step - loss: 2.8027e-04 - accuracy: 1.0000 - val_loss: 2.1734 - val_accuracy: 0.8319\n",
            "Epoch 21/150\n",
            "15/15 [==============================] - ETA: 0s - loss: 3.9453e-04 - accuracy: 1.0000\n",
            "Epoch 21: val_accuracy did not improve from 0.83193\n",
            "15/15 [==============================] - 7s 458ms/step - loss: 3.9453e-04 - accuracy: 1.0000 - val_loss: 2.0477 - val_accuracy: 0.8319\n",
            "Epoch 22/150\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-5f24a494a80a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfinal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfinal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_Train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_Train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_Test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_Test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msave_checkpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bvXBZvkxu6MH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "video_classification",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}